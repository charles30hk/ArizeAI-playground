{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f330c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"arize-phoenix[experimental,llama-index]\" \"openai>=1\" gcsfs nest_asyncio\n",
    "!pip install -qq  PyPDF2 langchain lang_chain_google_gen_ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aaa9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import phoenix as px\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b6c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.getenv(\"OPENAI_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77942c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_docs):\n",
    "    text=\"\"\n",
    "    for pdf in pdf_docs:\n",
    "        #print(pdf)\n",
    "        pdf_reader= PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text+= page.extract_text()\n",
    "    return  text\n",
    "\n",
    "\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_vector_store(text_chunks):\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "def get_conversational_chain():\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.3)\n",
    "\n",
    "    prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])\n",
    "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
    "\n",
    "    return chain\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7538842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://127.0.0.1:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]C:\\Users\\charl\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                   | 1/5 [00:08<00:32,  8.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What model is Gemini compared against? Gemini is compared against the following models:\n",
      "- GPT-4\n",
      "- PaLM 2-L\n",
      "- Claude 2\n",
      "- Inflection-2\n",
      "- Grok 1\n",
      "- LLAMA-2\n",
      "- MMLU\n",
      "\n",
      "Source: Gemini: A Family of Highly Capable Multimodal Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                  | 2/5 [00:15<00:23,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does Gemini compare to GPT4? Gemini Ultra improves its performance significantly from 84.0% with greedy sampling to 90.0% with uncertainty-routed chain-of-thought approach with 32 samples while it marginally improves to 85.0% with the use of 32 chain-of-thought samples only. In contrast, GPT-4‚Äôs performance improves from 84.2% with greedy sampling to 87.3% with uncertainty-routed chain-of-thought approach with 32 samples, but it already achieves these gains from using 32 chain-of-thought samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 3/5 [00:21<00:13,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the different models avaialble for Gemini? The Gemini family of models includes Gemini Nano-1, Gemini Nano-2, Gemini Pro, and Gemini Ultra.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 4/5 [00:29<00:07,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What benchmark was used to evaluate Gemini? Gemini was evaluated using the following benchmarks:\n",
      "- MMLU\n",
      "- GSM8K\n",
      "- MATH\n",
      "- American Mathematical Competitions\n",
      "- MGSM\n",
      "- XLSum\n",
      "- WikiLingua\n",
      "- WMT 23\n",
      "- VQAv2\n",
      "- TextVQA\n",
      "- DocVQA\n",
      "- ChartQA\n",
      "- InfographicVQA\n",
      "- MathVista\n",
      "- AI2D\n",
      "- MMMU\n",
      "- VATEX\n",
      "- VATEX ZH\n",
      "- YouCook2\n",
      "- NextQA\n",
      "- ActivityNet-QA\n",
      "- Perception Test MCQA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:36<00:00,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPT4 better than Gemini?Does Gemini support image generation? I do not have access to real-time information, therefore I cannot answer the question of whether GPT4 is better than Gemini.\n",
      "\n",
      "Gemini is able to output images natively, without having to rely on an intermediate natural language description that can bottleneck the model‚Äôs ability to express images. This uniquely enables the model to generate images with prompts using interleaved sequences of image and text in a few-shot setting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!pip install pyngrok\n",
    "#import getpass\n",
    "\"\"\"\n",
    "from pyngrok import ngrok, conf\n",
    "#print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\n",
    "conf.get_default().auth_token = \"2avo9mStYXThBAaeoguTfu98cjY_3DryQWqmQ9hS91HUDrHKR\"\n",
    "port = 37689\n",
    "# Open a ngrok tunnel to the HTTP server\n",
    "public_url = ngrok.connect(port).public_url\n",
    "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "# For the evaluation purpose, transform Google Gemini Report into text embedding \n",
    "raw_text = get_pdf_text([\"gemini_1_report.pdf\"])\n",
    "text_chunks = get_text_chunks(raw_text)\n",
    "get_vector_store(text_chunks)\n",
    "\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    "vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "vector_store.save_local(\"faiss_index2\")\n",
    "\n",
    "#Loading Vector Database   \n",
    "new_db = FAISS.load_local(\"faiss_index2\", embeddings)\n",
    "\n",
    "\n",
    "# Create a retriever object from the 'new_db' using the 'as_retriever' method.\n",
    "# This retriever is likely used for retrieving data or documents from the database.\n",
    "#faiss_retriever = new_db.as_retriever()\n",
    "\n",
    "#print(faiss_retriever)\n",
    "\n",
    "# Launch phoenix\n",
    "#session = px.launch_app(port=37689)\n",
    "session = px.launch_app()\n",
    "# Once you have started a Phoenix server, you can start your LangChain application with the OpenInferenceTracer as a callback. To do this, you will have to instrument your LangChain application with the tracer:\n",
    "\n",
    "from phoenix.trace.langchain import OpenInferenceTracer, LangChainInstrumentor\n",
    "\n",
    "# If no exporter is specified, the tracer will export to the locally running Phoenix server\n",
    "tracer = OpenInferenceTracer()\n",
    "LangChainInstrumentor(tracer).instrument()\n",
    "\n",
    "\n",
    "llm= ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3 , convert_system_message_to_human=True)\n",
    "#prompt_template = \"\"\"\n",
    "#    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "#    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "#    Context:\\n {context}?\\n\n",
    "#    Question: \\n{question}\\n\n",
    "#\n",
    "#    Answer:\n",
    "#    \"\"\"\n",
    "#prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])    \n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(),\n",
    ")\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "queries = [\n",
    "    \"What model is Gemini compared against?\",\n",
    "    \"How does Gemini compare to GPT4?\",\n",
    "    \"What are the different models avaialble for Gemini?\",\n",
    "    \"What benchmark was used to evaluate Gemini?\",\n",
    "    \"Is GPT4 better than Gemini?\"\n",
    "    \"Does Gemini support image generation?\"\n",
    "]\n",
    "\n",
    "for query in tqdm(queries):    \n",
    "    #docs = new_db.similarity_search(query)\n",
    "    temp=chain.run(query=query, return_only_outputs=True)\n",
    "    print(query, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97afe11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c0d8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∫ Opening a view to the Phoenix app. The app is running at http://127.0.0.1:6006/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://127.0.0.1:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2250996c490>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px.active_session().view()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19437c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414dac28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>span_kind</th>\n",
       "      <th>attributes.input.value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f6884afe-5289-4971-81d8-77a640f73f26</th>\n",
       "      <td></td>\n",
       "      <td>LLM</td>\n",
       "      <td>{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d748f26c-787d-4fe1-9b52-b5e8d0671417</th>\n",
       "      <td>LLMChain</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>{\"question\": \"Is GPT4 better than Gemini?Does ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aab9b9ac-0035-4ef3-8bfb-8fc326c8d1b3</th>\n",
       "      <td>StuffDocumentsChain</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>{\"input_documents\": [{\"page_content\": \"Megan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5ee842bd-53b0-4393-bdc8-2ba1c5abbb62</th>\n",
       "      <td>Retriever</td>\n",
       "      <td>RETRIEVER</td>\n",
       "      <td>Is GPT4 better than Gemini?Does Gemini support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6d412a9-24e1-4447-a667-f2ab987f0128</th>\n",
       "      <td>RetrievalQA</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>{\"query\": \"Is GPT4 better than Gemini?Does Gem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  span_kind  \\\n",
       "context.span_id                                                        \n",
       "f6884afe-5289-4971-81d8-77a640f73f26                             LLM   \n",
       "d748f26c-787d-4fe1-9b52-b5e8d0671417             LLMChain      CHAIN   \n",
       "aab9b9ac-0035-4ef3-8bfb-8fc326c8d1b3  StuffDocumentsChain      CHAIN   \n",
       "5ee842bd-53b0-4393-bdc8-2ba1c5abbb62            Retriever  RETRIEVER   \n",
       "e6d412a9-24e1-4447-a667-f2ab987f0128          RetrievalQA      CHAIN   \n",
       "\n",
       "                                                                 attributes.input.value  \n",
       "context.span_id                                                                          \n",
       "f6884afe-5289-4971-81d8-77a640f73f26  {\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...  \n",
       "d748f26c-787d-4fe1-9b52-b5e8d0671417  {\"question\": \"Is GPT4 better than Gemini?Does ...  \n",
       "aab9b9ac-0035-4ef3-8bfb-8fc326c8d1b3  {\"input_documents\": [{\"page_content\": \"Megan B...  \n",
       "5ee842bd-53b0-4393-bdc8-2ba1c5abbb62  Is GPT4 better than Gemini?Does Gemini support...  \n",
       "e6d412a9-24e1-4447-a667-f2ab987f0128  {\"query\": \"Is GPT4 better than Gemini?Does Gem...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spans_df = px.active_session().get_spans_dataframe()\n",
    "spans_df[[\"name\", \"span_kind\", \"attributes.input.value\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88bcc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.active_session())\n",
    "queries_df = get_qa_with_reference(px.active_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "593b0411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e138c4c850244ff18514276b459356f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/10 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0543c35c03a7414fb034ffc82f37bad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/20 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sending Evaluations:   0%|                                                                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Sending Evaluations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 93.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Sending Evaluations:   0%|                                                                      | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Sending Evaluations:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                              | 5/20 [00:00<00:00, 49.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "Sending Evaluations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 89.15it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import KNNRetriever\n",
    "from phoenix.experimental.evals import (\n",
    "    HallucinationEvaluator,\n",
    "    OpenAIModel,\n",
    "    QAEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    run_evals,\n",
    ")\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "\n",
    "\n",
    "eval_model = OpenAIModel(\n",
    "    model_name=\"gpt-4-1106-preview\",\n",
    ")\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "\n",
    "hallucination_eval_df, qa_correctness_eval_df = run_evals(\n",
    "    dataframe=queries_df,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df = run_evals(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]\n",
    "\n",
    "px.log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n",
    ")\n",
    "px.log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e20f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∫ Opening a view to the Phoenix app. The app is running at http://127.0.0.1:6006/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://127.0.0.1:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2250e232710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px.active_session().view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
